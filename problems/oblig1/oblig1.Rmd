---
title: "STK2100 Oblig 1"
author: "HÃ¥kon Kvernmoen"
date: "2/4/2021"
output: pdf_document
---

# Problem 1
## a)

First we need to load the data. The code sample for loading did not work for me (got a 400 bad request error). Assuming the datafile "nuclear.dat" is located in the same folder as this file, we load the data and attach it for easier use.

```{r}
nuclear = read.table("nuclear.dat", sep="\t", header=T)
attach(nuclear)
```

We notice that \texttt{pr}, \texttt{ne}, \texttt{ct}, \texttt{bw} and \texttt{pt} are binary variables, so we set them as factors.

```{r}
nuclear$pr = as.factor(nuclear$pr)
nuclear$ne = as.factor(nuclear$ne)
nuclear$ct = as.factor(nuclear$ct)
nuclear$bw = as.factor(nuclear$bw)
nuclear$pt = as.factor(nuclear$pt)
```

To investigate the data we plot the numerical features against each other. There seems to be some correlation between \texttt{date} and \texttt{t1}  

```{r}
plot(nuclear[,sapply(nuclear, is.numeric)])
```

## b)

The standard assumption on the noise terms \text{$\epsilon_i$} are.

1. The error terms are normally distributed with a mean of 0
2. The variance \text{$\sigma^2$} of this normal distribution is constant
3. The error terms are independent, \text{$\epsilon_i$} does not influence \text{$\epsilon_j$} 

Important?

We will now try to fit the model using all the features. As cost is always positive, we fit the log of the cost as a response variable. With \text{$y_{i}$} being the i'th observation of the cost, we will try to fit the model.

$$
log(y_i) = \beta_0 + \sum_{j=1}^{p} x_{i,j} + \epsilon_i 
$$
```{r}
all.fit = lm(log(cost)~., data = nuclear)
summary(all.fit)
```


## c)

We will now remove the term with the largest P-value. Observing the summary of the linear model, we see that \texttt{t1} has the largest P-value at 0.81610. This is sensible to do since the P-value is a measure of the correctness of the null-hypothesis (\text{$H_0$}). A large P-value as in this case indicates that there is a very little statistical basis for \texttt{t1} to be a good predictor for log(\texttt{cost}) and is thus neglected.

```{r}
all_no_t1.fit = lm(log(cost)~.-t1, data= nuclear)
summary(all_no_t1.fit)
```
We observe that there are some change in the P-values for a lot of the features after we excluded \texttt{t1}. This is probably due to correlation between the features. We would ideally have linearly independent explanatory variables. In example a change in \texttt{cap} should not influence any of the other explanatory variables, but this is not the case. On the other hand, the changes in P-values are not huge and the coefficients estimates seems relatively unchanged. In addition the standard error for the coefficients seems to decrease and we continue these modifications.

## d)
We now want to fit our model, remove the explanatory variable with a P-value larger than 0.05 and repeat this until we have a model where all explanatory variables have P-values smaller than 0.05. We then implement a backward substitution algorithm. We note that we do not want to remove the intercept even tough its P-value can be larger than 0.05.

```{r}
nuclear_backwards_sub <- data.frame(nuclear) 
for (i in 1:ncol(nuclear)) {
    fit <- lm(log(cost)~., data=nuclear_backwards_sub)
    # -1 since we don't want to remove intercept
    p_vals <- summary(fit)$coefficients[-1,4]
    max_idx <- as.integer(which.max(p_vals)) 
  
    if(p_vals[max_idx] < 0.05) {
      break
    } 
    else {
      # Add one since we don't want to remove targert variable (cost).
      nuclear_backwards_sub <- nuclear_backwards_sub[,-(max_idx+1)] 
    }
}

summary(fit)
```

We are then left with 4 explanatory variables. Two of them continues (\texttt{date, cap}) and two binary (\texttt{ne, pt}).

```{r}
par(mfrow=c(2,2))
plot(fit)
```

The model seems to fit reasonably well, but the simple linear model does not seem to capture the complexity of the data. From the residuals against fitted values plot we see that the residual mean seems to be around 0 for the lower fitted values, but for higher values log(\texttt{cost}) > 6.1 the mean seems to not be centered around 0. This is probably due to a small data set. The QQ-plot confirms this displaying that data point 7, 10 and 19 deviates a lot from the theoretical quantiles. From the Standardized residuals against leverage plot point 10 and 19 again shows a high deviance and should probably be considered outliers. Lastly point 26 deviates from the mean of zero as well as having a high leverage (influence on the model) and should also be considerd as an outlier. 

## e)
Finally we calculate the MSE

```{r}
MSE = (cost-exp(fitted.values(fit)))^2
MSE = mean(MSE)
MSE
```

This MSE is high and most of the contribution comes from the data points discussed in d). There is also a major drawback to this approach, as we have tested the correctness of our model (using the MSE) on the same data that we used to fit the model. The MSE resulted in a large value either way, but this should generally be avoided as more flexible models can follow the data used too closely (overfitting) and not represent the whole population. A better approach would be to split the data into a training and testing set, using only the training data to fit the model and then evaluate the correctness using the training data. Other techniques such as cross-validation can also be used.   

## f)

We begin with the likelihood function for a linear model with Gaussian error terms. Writing \text{$\mathbf{x}_i$} as  the i'th data point with all \text{$q-1$} 

$$
L(\boldsymbol{\theta}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left(-\frac{(y_i-\mathbf{x}_i^T \boldsymbol{\theta})^2}{2\sigma^2}\right)}
$$
To avoid the product sum we take the log of the likelihood, giving the log-likelihood. Note that \text{$\log$} refers to the natural logarithm.

$$
log(L(\boldsymbol{\theta})) = \sum_{i=1}^{n} -\frac{1}{2}log(\sigma^2) -  \frac{(y_i-\mathbf{x}^T \boldsymbol{\theta})^2}{2\sigma^2}  + const = -\frac{n}{2}log(\sigma^2) -  \frac{||\mathbf{y}-\mathbf{X}\boldsymbol{\theta}||^2}{2\sigma^2}  + const
$$
Inserting the maximum liklihood estimate \text{$\hat{\boldsymbol{\theta}}$} and estimating the variance \text{$\hat{\sigma^2} = ||\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\theta}}||^2/n$}, the log likelihood evaluated at the at the maximum likelihood estimate becomes.

$$
log(L(\hat{\boldsymbol{\theta}})) = -\frac{n}{2} log(\hat{\sigma^2}) - \frac{n\sigma^2}{2\sigma^2} + const = -\frac{n}{2}log(\hat{\sigma^2}) - \frac{n}{2} + const.  
$$
As we will use this to preform variable selection, we absorb \text{$n$} into the const term, as it will be constant no matter the amount of variables we include in the model. Rearranging:

$$
-2log(\hat{\boldsymbol{\theta}}) = nlog(\sigma^2) + const
$$
Inserting this into the two equation for BIC and AIC will retrive the result presented in AIC.LM and BIC.LM

## g)

We first use backward selection with the AIC.
```{r}
library("MASS")
stepAIC(all.fit, direction = "backward")
```
Then the same with BIC
```{r}
n = nrow(nuclear)
stepAIC(all.fit, direction="backward", k=log(n))
```

Both AIC and BIC starts with the full model and tries to remove one explanatory variable. The model with the highest likelihood is chosen and the cycle continues until there is no improvement in AIC-value. The two models we ended up with was

AIC:  \texttt{date,t2, cap, pr, ne, ct, cum.n, pt}

BIC: \texttt{date, cap, ne, ct, pt}

## h)

## g)
P-values does not take complexity into account.
