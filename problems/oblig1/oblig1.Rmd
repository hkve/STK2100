---
title: "STK2100 Oblig 1"
author: "HÃ¥kon Kvernmoen"
date: "2/4/2021"
output: pdf_document
---

# Problem 1
## a)

First we need to load the data. The code sample for loading did not work for me (got a 400 bad request error). Assuming the data file "nuclear.dat" is located in the same folder as this file, we load the data and attach it for easier use.

```{r}
nuclear = read.table("nuclear.dat", sep="\t", header=T)
attach(nuclear)
```

We notice that \texttt{pr}, \texttt{ne}, \texttt{ct}, \texttt{bw} and \texttt{pt} are binary variables, so we set them as factors.

```{r}
nuclear$pr = as.factor(nuclear$pr)
nuclear$ne = as.factor(nuclear$ne)
nuclear$ct = as.factor(nuclear$ct)
nuclear$bw = as.factor(nuclear$bw)
nuclear$pt = as.factor(nuclear$pt)
```

To investigate the data we plot the numerical features against each other. There seems to be some correlation between \texttt{date} and \texttt{t1}  

```{r}
plot(nuclear[,sapply(nuclear, is.numeric)])
```

## b)

The standard assumption on the noise terms \text{$\epsilon_i$} are.

1. The error terms are normally distributed with a mean of 0
2. The variance \text{$\sigma^2$} of this normal distribution is constant
3. The error terms are independent, \text{$\epsilon_i$} does not influence \text{$\epsilon_j$} 

As to which assumption is most important, I think a combination of 1 and 2. Assumption 1 is import since many of the statistical tools we use are built around the error terms being normally distributed. In addition if the mean was not zero, there might be some systematic error in our data, leading to an incorrect model. Assumption 3 is also important for the same reason. I would argue that assumption 2 is less important, because if lets say the variance increases, given that the mean is zero and the error terms are independently Gaussian, the error would just become larger and thus a "random" error instead of a systematic error.  

We will now try to fit the model using all the features. As cost is always positive, we fit the log of the cost as a response variable. With \text{$y_{i}$} being the i'th observation of the cost, we will try to fit the model.

$$
log(y_i) = \beta_0 + \sum_{j=1}^{p} x_{i,j} + \epsilon_i 
$$
```{r}
all.fit = lm(log(cost)~., data = nuclear)
summary(all.fit)
```


## c)

We will now remove the term with the largest P-value. Observing the summary of the linear model, we see that \texttt{t1} has the largest P-value at 0.81610. This is sensible to do since the P-value is a measure of the correctness of the null-hypothesis (\text{$H_0$}). A large P-value as in this case indicates that there is a very little statistical basis for \texttt{t1} to be a good predictor for log(\texttt{cost}) and is thus neglected.

```{r}
all_no_t1.fit = lm(log(cost)~.-t1, data= nuclear)
summary(all_no_t1.fit)
```
We observe that there are some change in the P-values for a lot of the features after we excluded \texttt{t1}. This is probably due to correlation between the features. We would ideally have linearly independent explanatory variables. In example a change in \texttt{cap} should not influence any of the other explanatory variables, but this is not the case. On the other hand, the changes in P-values are not huge and the coefficients estimates seems relatively unchanged. In addition the standard error for the coefficients seems to decrease and we continue these modifications.

## d)
We now want to fit our model, remove the explanatory variable with a P-value larger than 0.05 and repeat this until we have a model where all explanatory variables have P-values smaller than 0.05. We then implement a backward substitution algorithm. We note that we do not want to remove the intercept even tough its P-value can be larger than 0.05.

```{r}
nuclear_backwards_sub <- data.frame(nuclear) 
for (i in 1:ncol(nuclear)) {
    p_values.fit <- lm(log(cost)~., data=nuclear_backwards_sub)
    # -1 since we don't want to remove intercept
    p_vals <- summary(p_values.fit)$coefficients[-1,4]
    max_idx <- as.integer(which.max(p_vals)) 
  
    if(p_vals[max_idx] < 0.05) {
      break
    } 
    else {
      # Add one since we don't want to remove targert variable (cost).
      nuclear_backwards_sub <- nuclear_backwards_sub[,-(max_idx+1)] 
    }
}

summary(p_values.fit)
```

We are then left with 4 explanatory variables. Two of them continues (\texttt{date, cap}) and two binary (\texttt{ne, pt}).

```{r}
par(mfrow=c(2,2))
plot(p_values.fit)
```

The model seems to fit reasonably well, but the simple linear model does not seem to capture the complexity of the data. From the residuals against fitted values plot we see that the residual mean seems to be around 0 for the lower fitted values, but for higher values log(\texttt{cost}) > 6.1 the mean seems to not be centered around 0. This is probably due to a small data set. The QQ-plot confirms this displaying that data point 7, 10 and 19 deviates a lot from the theoretical quantiles. From the Standardized residuals against leverage plot point 10 and 19 again shows a high deviance and should probably be considered outliers. Lastly point 26 deviates from the mean of zero as well as having a high leverage (influence on the model) and should also be considerd as an outlier. 

## e)
Finally we calculate the MSE

```{r}
MSE = (cost-exp(fitted.values(p_values.fit)))^2
MSE = mean(MSE)
MSE
```

This MSE is high and most of the contribution comes from the data points discussed in d). There is also a major drawback to this approach, as we have tested the correctness of our model (using the MSE) on the same data that we used to fit the model. The MSE resulted in a large value either way, but this should generally be avoided as more flexible models can follow the data used too closely (overfitting) and not represent the whole population. A better approach would be to split the data into a training and testing set, using only the training data to fit the model and then evaluate the correctness using the training data. Other techniques such as cross-validation can also be used.   

## f)

We begin with the likelihood function for a linear model with Gaussian error terms. Writing \text{$\mathbf{x}_i$} as  the i'th data point with all \text{$q$} features. 

$$
L(\boldsymbol{\theta}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{\left(-\frac{(y_i-\mathbf{x}_i^T \boldsymbol{\theta})^2}{2\sigma^2}\right)}
$$
To avoid the product sum we take the log of the likelihood, giving the log-likelihood. Note that \text{$\log$} refers to the natural logarithm.

$$
log(L(\boldsymbol{\theta})) = \sum_{i=1}^{n} -\frac{1}{2}log(\sigma^2) -  \frac{(y_i-\mathbf{x}^T \boldsymbol{\theta})^2}{2\sigma^2}  + const = -\frac{n}{2}log(\sigma^2) -  \frac{||\mathbf{y}-\mathbf{X}\boldsymbol{\theta}||^2}{2\sigma^2}  + const
$$
Inserting the maximum liklihood estimate \text{$\hat{\boldsymbol{\theta}}$} and estimating the variance \text{$\hat{\sigma^2} = ||\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\theta}}||^2/n$}, the log likelihood evaluated at the at the maximum likelihood estimate becomes.

$$
log(L(\hat{\boldsymbol{\theta}})) = -\frac{n}{2} log(\hat{\sigma^2}) - \frac{n\sigma^2}{2\sigma^2} + const = -\frac{n}{2}log(\hat{\sigma^2}) - \frac{n}{2} + const.  
$$
As we will use this to preform variable selection, we absorb \text{$n$} into the const term, as it will be constant no matter the amount of variables we include in the model. Rearranging:

$$
-2log(\hat{\boldsymbol{\theta}}) = nlog(\sigma^2) + const
$$
Inserting this into the two equation for BIC and AIC will retrive the result presented in AIC.LM and BIC.LM

## g)

We first use backward selection with the AIC.
```{r}
library("MASS")

AIC.fit = lm(log(cost)~.,data = nuclear)
AIC.fit = stepAIC(AIC.fit, direction = "backward")
```
Then the same with BIC
```{r}
AIC.fit$coefficients

n = nrow(nuclear)
BIC.fit = lm(log(cost)~., data=nuclear)
BIC.fit = stepAIC(BIC.fit, direction="backward", k=log(n))
```

Both AIC and BIC starts with the full model and tries to remove one explanatory variable. The model with the highest likelihood is chosen and the cycle continues until there is no improvement in AIC-value. The two models we ended up with was

AIC:  \texttt{date,t2, cap, pr, ne, ct, cum.n, pt}

BIC: \texttt{date, cap, ne, ct, pt}

## h)

Both AIC and BIC contain to parts. Both include a \text{$n log(\hat{\sigma}^2)$} term, measuring how good the model fits the data, and is equal in both procedures. The difference is in the second terms, \text{$2(q+2)$} and \text{$log(n) (q+2)$} for AIC and BIC respectively. This terms act as a regulator for complexity. Models with high complexity (i.e. many explanatory variables) will be penalized. If we leave one explanatory variable out, \text{$n log(\hat{\sigma}^2)$} will be equal for the two procedures, assuming we have left out the same explanatory variable. The two complexity regulator terms (\text{$2(q+2)$} and \text{$log(n) (q+2)$}) equal for one removal, and does not discriminate which explanatory variables left out. Thus starting at maximum complexity (including all explanatory variables), the order of what explanatory variable to remove, should be equal for the two procedures. The difference is in how much we should penalize the model. We have 32 observations, so \text{$log(32) \approx 3.46 > 2$}, thus for this dataset, BIC will punish complex models harder than AIC, which coincides with BIC resulting in fewer explanatory variables than AIC.  

## i)
Selecting a model using the "arbitrary" criteria of a p-value less than 0.05 might lead to some problems. Our sample size is low (32 observations) and we have quite a lot of explanatory variables relative to samples. P-values are dependent on sample size, so some variables that should have a p-value under this threshold might be picked out even though they are statistically significant (NB: only if the \text{$H_0$} hypothesis is false). The stepwise AIC and BIC does not intringsantly have this problem as we are only trying to reduce AIC/BIC on the same data set. These step-wise schemes should be used carefully tough, since overfitting can be a problem. Both AIC and BIC tries many different models on the same data set and only reports if improvements are made and thus prone to data dredging.          


<!-- BIC might choose a too simple model. -->
<!-- P-values does not take complexity into account. -->

## j)
We define \text{$Z = \mu + \sigma W$} such that \text{$E[W] = 0, Var[W]=1$} (the standard normal). Since Z/W is drawn from a continuous normal distribution write the expectation value as.

$$
E[e^Z] = E[e^{\mu+\sigma W}] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{\mu + \sigma W}e^{-\frac{W^2}{2}} dW = \frac{e^\mu}{2\pi}\int_{-\infty}^{\infty} e^{\sigma W}e^{-\frac{W^2}{2}} dW
$$
Now we want to complete the square in the exponent. 

$$
\sigma W- \frac{W^2}{2} = \frac{1}{2}\left( W^2 - 2\sigma W + \sigma^2 \right) - \frac{\sigma^2}{2} = \frac{1}{2} \left( W-\sigma\right)^2 - \frac{\sigma^2}{2}
$$

Inserting this into our integral

$$
E[Z] = \frac{e^\mu}{2\pi}\int_{-\infty}^{\infty} e^{-\frac{1}{2}\left(W-\sigma \right)^2 + \frac{1}{2}\sigma^2} dW = \frac{e^{\mu + \frac{1}{2}\sigma^2}}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-\frac{(W-\sigma)^2}{2}} dW = e^{\mu+\frac{1}{2}\sigma^2}
$$

When estimating the \text{$\hat{\eta}$} we can consider two options, one reflecting the calculation just done \text{$\hat{\eta}_2 = e^{\hat{\theta}+\frac{1}{2}\hat{\sigma}^2}$} and one without the standard deviation estimate \text{$\hat{\eta}_1 = e^{\hat{\theta}}$}. I think \text{$\hat{\eta}_1$} is the preferred choice here, as the estimated coefficiants should be indipendent of the standard deviate of the Gaussian noise (assuming heteroskedasticity). 

## k)
We now define the new data point and use the 3 different models to make a prediction.

```{r}
nuclear.new = data.frame(date=70.0, t1=13, t2=50, cap = 800, pr=1, ne=0, ct=0, bw=1, cum.n=8, pt=1)

nuclear.new$pr = as.factor(nuclear.new$pr)
nuclear.new$ne = as.factor(nuclear.new$ne)
nuclear.new$ct = as.factor(nuclear.new$ct)
nuclear.new$bw = as.factor(nuclear.new$bw)
nuclear.new$pt = as.factor(nuclear.new$pt)

p_values.pred = predict.lm(p_values.fit, nuclear.new, se.fit=TRUE)
AIC.pred = predict.lm(AIC.fit, nuclear.new, se.fit=TRUE)
BIC.pred = predict.lm(BIC.fit, nuclear.new, se.fit=TRUE)

sprintf("P-value estimate: %f SE: %f", exp(p_values.pred$fit),exp(p_values.pred$se.fit))
sprintf("AIC estimate: %f SE: %f", exp(AIC.pred$fit),exp(AIC.pred$se.fit))
sprintf("BIC estimate: %f SE: %f", exp(BIC.pred$fit),exp(BIC.pred$se.fit))
```

The results are summerised in table 1.

\begin{table}[h!]
\centering
\caption{Different model predictions and standard errors}
\begin{tabular}{|l|l|l|}
\hline
Model & Prediction & SE \\ \hline
P-values & 356.490489   & 1.122362 \\ \hline
AIC & 390.897773 & 1.161922 \\ \hline
BIC & 360.778910 & 1.117556 \\
\hline
\end{tabular}
\end{table}

Overall, the standard error seems to be relatively equal across all the models. The prediction however, differs. The P-value and BIC a approaches seems to agree, giving almost equal predictions. The AIC model on the other hand, differs with about 30-35 from the two other predictions. Both the BIC and P-value method resulted in fewer explanatory variables giving a simpler model, while the AIC model had more complexity. As the BIC and P-values models agreed, this might suggest a too complex model (using AIC) to predict log(\texttt{cost}). However without knowing what value of \texttt{cost} this new data-point gives, makes model selection difficult and more data-points should also be considered.   

# Problem 2
## a)

The following code splits the data set (once, randomly) into a training and test set of two equal parts. Using the same stepAIC scheme as before, but opting for forward selection compared to backward selection. The lowest complexity of the model is set as one the simple constant term, while the highest complexity is set to include all the explanatory variables. The stepAIC scheme is preformed 10 times, with \text{$i = 0,1,...,10$}. I here represents how many steps the AIC should take (how any explanatory variables should potentially be added) starting from the lowest complexity model. The RMSE from both the log(\texttt{cost}) and \texttt{cost} is calculated on the test set and then saved (for each i) and then finally plotted against the number of explanatory variables included. 

Running the program creates the following plot, suggesting that the best model only has one parameter. There is some problems with this however, as there seems to be some randomness involved. Running the code multiple times produces different plots, but the minimum at 1 seems to be relatively stable. This is due to the test/train split being random. Every time we run the code, we train the model many times and pick the one that best fits the random testing set. The problem with this is that we might not get the best model, but simply the best model for that particularly testing set. The amount of different models tested is very large, so this does seem to be a problem.

```{r, echo=T,results='hide',fig.keep='last'}
n = nrow(nuclear)
ind = sample(1:n,n/2,replace=FALSE)
RMSE.test1 = rep(NA,11) 
RMSE.test2 = rep(NA,11) 
model_narrow = lm(log(cost) ~ 1, data = nuclear)
model_wide = lm(log(cost) ~ ., data = nuclear)

for(i in 0:10)
{
 fit = stepAIC(model_narrow, direction="forward", steps=i,data=nuclear[ind,],
               scope=list(lower=model_narrow, upper=model_wide), k = 0)
 pred = predict(fit,nuclear[-ind,])
 RMSE.test1[i+1] = sqrt(mean((log(nuclear$cost)-pred)^2))
 RMSE.test2[i+1] = sqrt(mean((nuclear$cost-exp(pred))^2))
}
par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for z axis
plot(0:10,RMSE.test1,main="Train/Test split",xlab="Complexity",ylab="RMSE1",type="l") # first plot
par(new = TRUE)
plot(0:10,RMSE.test2, type = "l", axes = FALSE, bty = "n", xlab = "", ylab = "",col=2)
axis(side=4, at = pretty(range(RMSE.test2)))
mtext("RMSE2", side=4, line=3)
```

## b)
The next code section uses 10-fold cross-validation. The same steps over \text{$i$} are done and for each step a model is selected (using stepAIC on the whole data set). Then the best model is chosen (for each step), the RMSE errors for both log(\texttt{cost}) and \texttt{cost} are calculated, but this time using 10-fold CV. The data-set is split into 10 parts, where 9 of these parts are used to train the optimal model found by the stepAIC. Then the RMSE is calculated on the last part (the hold out fold). This repeats until every part has been the hold out fold once, and the mean RSME over the 10-folds are calculated.

Running this code several times yields relatively stable results, but there is some randomness involved. We are interested in the minimum of these two curves, which seems to be around 4-5 explanatory variables. The randomness again come from the division of the folds, as this is done randomly. The trend is relativly stable, but it is dificult to give a definit minimum.

```{r, echo=T,results='hide',fig.keep='last'}
library(lmvar)
RMSE.cv1 = rep(0,10)
RMSE.cv2 = rep(0,10)
for(i in 0:10)
{
fit = stepAIC(model_narrow, direction="forward", steps=i,k=0,
        scope=list(lower=model_narrow, upper=model_wide),trace=0)
 fit = lm(formula(fit),data=nuclear,x=TRUE,y=TRUE)
 #Note: the k in the command below has a different meaning than k above!!!
 RMSE.cv1[i+1] = cv.lm(fit,k=10)$MSE_sqrt$mean
 RMSE.cv2[i+1] = cv.lm(fit,k=10,log=TRUE)$MSE_sqrt$mean
}

par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for z axis
plot(0:10,RMSE.cv1,xlab="Complexity",ylab="RMSE1",type="l") # first plot
par(new = TRUE)
plot(0:10,RMSE.cv2, main="10-fold CV",type = "l", axes = FALSE, bty = "n", xlab = "", ylab = "",col=2)
axis(side=4, at = pretty(range(RMSE.cv2)))
mtext("RMSE2", side=4, line=3)
```

## c)

We now want to implement Leave-one-out cross validation (LOOCV). From the \texttt{boot} library we make use of the \text{glm} function. This can be fed into a \texttt{cv.glm} function that be default uses LOOCV. Since we are fitting a ordinary least squares model we can make use of the fantastic shortcut 

$$
MSE = \frac{1}{n} \sum_{i=1}^{n}\left( \frac{y_i - \hat{y_i}}{1-h_{i}} \right)^2 \hspace{40px} RMSE = \sqrt{MSE} 
$$
Where \text{$h_i$} is the leverage. This actually makes the RMSE calculations a lot faster, since there is no need to fit the same model n times. Running the code many times confirms that these results are more stable than 10-fold CV and way more stable than Train/Test split. This yields a prefered model complexity of 4 explanatory variables.

```{r}
library(boot)
RMSE.LOOCV1 = rep(0,11)
RMSE.LOOCV2 = rep(0,11)
for(i in 0:10) {
  fit = stepAIC(model_narrow, direction="forward", steps=i,k=0,
        scope=list(lower=model_narrow, upper=model_wide),trace=0)
 fit = glm(formula(fit),data=nuclear,x=TRUE,y=TRUE)
 
 RMSE.LOOCV1[i+1] = sqrt(cv.glm(nuclear ,fit)$delta[1])
 
 nuclear.diag = glm.diag(fit)
 yhat = exp(fitted(fit))
 y= exp(fit$y)
 RMSE.LOOCV2[i+1] = sqrt(mean((y-yhat)^2 / (1-nuclear.diag$h)^2))
}

par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for z axis
plot(0:10,RMSE.LOOCV1,xlab="Complexity",ylab="RMSE1",type="l") # first plot
par(new = TRUE)
plot(0:10,RMSE.LOOCV2, main="LOOCV",type = "l", axes = FALSE, bty = "n", xlab = "", ylab = "",col=2)
axis(side=4, at = pretty(range(RMSE.LOOCV2)))
mtext("RMSE2", side=4, line=3)
```

# Problem 3

## a)
For the two models to be equal, the response \text{$y_i$} should be equal. We then write the following relationship between the \text{$\beta$} and \text{$\alpha$} models. 


\begin{align}
\sum_{j=1}^{K}\beta_0 + \sum_{j=2}^K \beta_j x_{ij} &= \sum_{j=1}^K \alpha_j x_{ij} \nonumber \\
\sum_{j=2}^K \beta_j x_{ij} &= \sum_{j=1}^K \left( \alpha_j x_{ij} - \beta_0\right) \nonumber
\end{align}

The i'th row of \text{$\mathbf{X}$} contains all zeroes, except the one non-zero element corresponding to category \text{$c_i = j$}. Looking at the relationship we first set \text{$x_{i1} = 1$}, and thus all other \text{$x_{ij} = 0$ for $ j = 2, 3, ... K$}. If the two models are equal, this implies that \text{$\beta_0 = \alpha_1$} since the rhs of the last equality is zero. Setting \text{$x_{i2} = 1$} giving all other \text{$x_{ij} = 0$ for $ j = 1, 3, 4, ... K$} and thus \text{$\beta_2 = \alpha_2 - \beta_0$}. We can do this in general setting \text{$x_{il} = 1$}  thus \text{$x_{ij} = 0$ for $ j = 1, 2, ... , l-1, l+1, ... ,K$} giving the relationship \text{$\beta_j = \alpha_j - \beta_0 = \alpha_j - \alpha_1$}. The relationship between the \text{$\alpha$} and \text{$\beta$} coefficients is thus.


$$
\beta_j = \begin{cases}
\alpha_1 \quad\quad\quad\hspace{3px}\text{if}\quad j=0 \\
\alpha_j - \alpha_0 \quad\text{if}\quad j=2,3,...,K
\end{cases}
$$
The interpretation of this is that the \text{$\beta$} model has category \text{$c_j=1$} as a baseline, corresponding to the coefficient that is always present no matter the value of \text{$x_{ij}$} (\text{$\beta_0$}). For this model all other \text{$\beta_j$} is the response \text{$y_i$} from category \text{$c_i = j$} \textit{relative} to category \text{$c_i = 1$}.

The \text{$\alpha$} model on the other hand, does not have this baseline value. The coefficients \text{$\alpha_j$} corresponding to category \text{$c_i = j$} is the response \text{$y_i$}, with no baseline (or zero as a baseline).  

## b)
The design matrix will have the form 
$$
\mathbf{X} =
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1k} \\
x_{21} & x_{22} & \dots & x_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & \dots & \dots & x_{nk}
\end{bmatrix}
$$
We can then look at an arbitrary element of the product matrix \text{$\mathbf{X}^T \mathbf{X}$}. This will result in the sum of the products of the i'th row in \text{$\mathbf{X}^T$} and the j'th row in \text{$\mathbf{X}$}

$$
[\mathbf{X}^T \mathbf{X}]_{ij} = \sum_{l = 1}^{n} x_{il}x_{lj} 
$$

Both \text{$x_{il}$} and \text{$x_{lj}$} can be equal to 1, but if \text{$i \neq j$} they will never both be one, and thus the sum equals 0 (for \text{$i \neq j$}). For \text{$i=j$}, \text{$x_{il} = x_{li} = 1$} if \text{$c_i = j$} and thus the sum will count the number of appearances of variable \text{$c_i$}, the final result is then 

$$
[\mathbf{X}^T \mathbf{X}]_{ij} = \begin{cases} 
n_j \quad\text{if}\quad i=j \quad \text{where}\quad j=1,2,...,K\\
0   \quad\hspace{6px}\text{if}\quad i\neq j
\end{cases}
$$
We will also calculate \text{$\mathbf{X}^T \mathbf{y}$}. We can again look at an arbitrary element of the vector \text{$\mathbf{X}^T y$}, summing over the product of each row in \text{$\mathbf{X}^T$} and \text{$\mathbf{y}$}. 

$$
[\mathbf{X}^T \mathbf{y}]_{j} = \sum_{l=1}^{n} x_{li} y_{l}
$$
Again \text{$x_{li}$} will be 1 if \text{$c_i = j$} and zero otherwise, thus effectively each row is the sum of the response \text{$y_j$} corresponding to this explanatory variable.

To find the least-squares estimate we make use of the general formula \text{$\hat{\boldsymbol{\alpha}} = (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$}. As \text{$\mathbf{X}^T\mathbf{X}$} is a diagonal matrix, inverting this will only result in a diagonal matrix with elements \text{$[(\mathbf{X}^T \mathbf{X})^{-1}]_{jj} = 1/n_j$}. We have already calculated \text{$\mathbf{X}^T \mathbf{y}$} so the least-squares estimate is then

$$
[\hat{\boldsymbol{\alpha}}]_{j} = \frac{1}{n_j} \sum_{l=1}^{n} x_{li}y_l
$$

\text{$\hat{\boldsymbol{\alpha}}$} is then a vector where each row \text{$j$} is the mean of the response corresponding to category \text{$c_i = j$}. 

## c)

Using the relationship from a) we find that the first coefficient \text{$\hat{\beta_0}$} is.

$$
[\hat{\boldsymbol{\beta}}]_0 = [\hat{\boldsymbol{\alpha}}]_1 = \frac{1}{n_1} \sum_{l=1}^{n} x_{l1}y_{l}
$$

The other coefficients then follow 

$$
[\hat{\boldsymbol{\beta}}]_j = [\hat{\boldsymbol{\alpha}}]_j - [\hat{\boldsymbol{\alpha}}]_1= \frac{1}{n_j} \sum_{l=1}^{n} x_{lj}y_{l} -\frac{1}{n_1} \sum_{l=1}^{n} x_{l1}y_{l}
$$

Since this is a 1-to-1 mapping of the coefficients, this also has to be the least-squares estimate for \text{$\boldsymbol{\beta}$}.

## d)
Since the two models should be equal, we demand that the sum over all the coefficients should be equal. 

$$
\sum_{j=1}^{K} \alpha_j = \sum_{j=1}^{K} \left( \gamma_0 + \gamma_j \right) = k\gamma_0 + \sum_{j=1}^{K} \gamma_j
$$
The sum over all \text{$\gamma_j$} is equal to zero by assumption, and thus the baseline for this \text{$\gamma$} model is.

$$
\gamma_0 = \frac{1}{k} \sum_{j=1}^{K} \alpha_j = \bar{\alpha}
$$
 Representing the average over all the \text{$\alpha$} coefficients. By the same argument as in a), we set one \text{$x_{il} = 1$} with all other \text{$x_{ij}$ for $j = 1, 2, ... , l-1, l+1, ..., K$} equal to zero and find the relation.
 
\begin{align} 
\alpha_l &= \gamma_0 + \gamma_l \nonumber \\
\gamma_l &= \alpha_l - \gamma_0 = \alpha_l - \bar{\alpha} \nonumber
\end{align}

The \text{$\gamma_l$}'s are now the contribution to the response, relative to the average response, not some arbitrary category \text{$c_i = 1$} as for the \text{$\beta$}'s.
 
## e)
Again the code provided did not work for me, so assuming the data file \texttt{fe.txt} is located in the same folder we load in the data and preform fit1.
 
```{r}
Fe = read.table("fe.txt", header=T, sep=",")
fit1= lm(Fe~form+0, data=Fe)
summary(fit1)
```
 
This is not a good idea, since we want R to recognize that form is a categorical variable. If not done, it will try to fit \texttt{Fe} against the numerical x-values 1,2,3,4. Thus we make sure the \texttt{form} variable is a regonzied as a factor. 

```{r}
Fe$form = as.factor(Fe$form)
fit1 = lm(Fe~form+0, data=Fe)
summary(fit1)
```
 This result is more reasonable as each of the 4 categories has its own coefficient. The actual numerical values 1,2,3,4 does not effect the model which was the case before we made them categorical. In the \texttt{lm} command, we enforced the fit to not include a intercept (+0). This implies that this fit corresponds to a model that does not have any baseline (or a baseline of zero). It is thus the \text{$\alpha$} model.
 
## f)
 
```{r}
options()$contrasts
options(contrasts=c("contr.treatment", "contr.treatment"))
fit2 = lm(Fe~form,data=Fe)
summary(fit2)

options(contrasts=c("contr.sum", "contr.sum"))
options()$contrasts
fit3 = lm(Fe~form,data=Fe)
summary(fit3)
```
We recognize that the first coefficient of \texttt{fit1} (\text{$\alpha_1$}) is equal to the intercept of \texttt{fit2}, indicating that \texttt{fit2} is the \text{$\beta$} model. In addition it is missing the \texttt{form1} coefficient. To confirm this calculate the mean of the coefficients of \texttt{fit1} and see that this is equal to 28.64, exactly the intercept of \texttt{fit3}, indicating that this is the \text{$\gamma$} model. The coefficients are presented in table 2. We can calulate the form4 coefficient.

```{r}
form4_gamma = - sum(fit3$coefficients[-1])
form4_gamma
```

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
          & fit1 ($\alpha$) & fit2 ($\beta$) & fit3 ($\gamma$) \\ \hline
Intercept & ---             & 26.08          & 28.64           \\ \hline
form1     & 26.08           & ---            & -2.56           \\ \hline
form2     & 24.69           & -1.39          & -3.95           \\ \hline
form3     & 29.95           & 3.87           & 1.31            \\ \hline
form4     & 33.84           & 7.76           & 5.4            \\ \hline
\end{tabular}
\caption{The different coefficients from the 3 fits}
\end{table}

To check if the relations hold.

```{r, results='hold'}
alpha = unname(fit1$coefficients)
beta = unname(fit2$coefficients)
gamma = c(unname(fit3$coefficients), form4_gamma)

print("Alpha-beta relations")
sprintf("beta(0) = %.2f alpha(1) = %.2f", beta[1], alpha[1])
for(i in 2:length(alpha)) {
  temp = sprintf("beta(%d) = %.2f alpha(%d)-beta(0) = %.2f", i, beta[i], i, alpha[i]-beta[1])
  print(temp)
}

print("Alpha-gamma relations")
sprintf("mean(alpha) = %.2f gamma(0) = %.2f", mean(alpha), gamma[1])
for(i in 1:(length(alpha))) {
  temp = sprintf("alpha(%d) = %.2f gamma(%d)+gamma(0) = %.2f", i, alpha[i], i, gamma[1]+gamma[i+1])
  print(temp)
}
```
 And they seem to hold. 
 
## g)

After my understanding, this task asks to choose one of the models and preform some sort of hypothesis testing to see if some of the categories can be neglected. 

Looking at \texttt{fit3} we see that \texttt{form3} ($\gamma_3$) has a large std. error compared to its coefficient and a large p-value. We then propose the following hypnosis. 

\begin{align}
H_0:& \gamma_3 = 0 \text{  and  } \gamma_j \neq 0 \text{  for  } j = 1,2,4 \nonumber \\
H_a:& \gamma_j \neq 0 \text{  for  } j = 1,2,3,4 \nonumber
\end{align}

We will use the F-statistic to determine correlation. We already have the F-statistic under $H_a$, $F_a = 10.85$. Then we remove category $\gamma_3$ from the model and fit again. 

```{r, results='hold'}
options()$contrasts
options(contrasts=c("contr.sum", "contr.sum"))
fit3.H0 = lm(Fe~form, data=Fe[Fe$form != 3,])
summary(fit3.H0)
```
 
This yields a F-statistic under $H_0$, $F_0 = 13.4$. Since $F_0 > F_a$ we do not reject $H_0$ and it seems that category 3=magnetite is not statistically significant to predict iron content. 

## h)
The hypothesis testing preformed g) indicates that magnetite is not a good predictor to predict iron content. We could try to preform  hypothesis testing on the other models and especially on the $\alpha$ model that at face value seems to show a high significance for all variables.
